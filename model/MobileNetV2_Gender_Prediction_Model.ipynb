{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MobileNetV2 Gender Prediction Model",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofjgm0JuEbXG",
        "colab_type": "text"
      },
      "source": [
        "# Gender Prediction using MobileNetV2\n",
        "\n",
        "By Abhishek Chatterjee\n",
        "(abhishekchatterjeejit@gmail.com)\n",
        "\n",
        "**The aim of this project is to make a computer program to detect the gender of a person based on the single image of his/her face. This project is using the MobileNetV2 deep learning CNN architecture to predict it. The dataset that is used to train is a mix of the IMDB WIKI dataset and Selfie Dataset.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qC1ItRMZFbSi",
        "colab_type": "text"
      },
      "source": [
        "## Dependencies\n",
        "\n",
        "In the first step, we wil import the dependencies that we need for this project."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKmXTPpeCUcy",
        "colab_type": "code",
        "outputId": "1a3db78f-0808-4b3c-9c15-06376ae910c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Dependencies\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten\n",
        "from keras.applications import MobileNetV2\n",
        "from keras import optimizers\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "965oWlMrGb5v",
        "colab_type": "text"
      },
      "source": [
        "## Connecting Google Drive\n",
        "\n",
        "As I'm running it on Google Colab, and my dataset is stored into Google Drive, so I need to connect Colab with Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DV9pbw4gGqmE",
        "colab_type": "code",
        "outputId": "f6968469-8607-4e27-840d-704a5cc73198",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btcrbaesHRwq",
        "colab_type": "text"
      },
      "source": [
        "## Unzipping the dataset\n",
        "\n",
        "In Google Drive, the dataset is stored as a zip file. So before using it, I need to unzip it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tee1RLZpHdVd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip -qq 'drive/My Drive/dataset/imdb+wiki+selfie.zip' -d ./"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qUihhGFITe-",
        "colab_type": "text"
      },
      "source": [
        "There are three files in the dataset.\n",
        "\n",
        "\n",
        "*   images/ folder - This folder contains the original images\n",
        "*   gender.csv - This CSV file contains the meta information for the dataset (gender and image name)\n",
        "*   age.csv - This CSV file contains the meta information for the age dataset (not needed here)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixqmbA_HG3lF",
        "colab_type": "text"
      },
      "source": [
        "## Reading the Dataset\n",
        "\n",
        "In this step, I will read the dataset, stored in CSV format. To read the dataset, I will use the pandas read_csv method.\n",
        "\n",
        "Note: The entire the dataset is already preprocessed and cleaned. Please check the preprocessing code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjuGyea0FKQX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gender_data = pd.read_csv('./gender.csv')\n",
        "\n",
        "# Fir testing im just using 10% of the data\n",
        "gender_data = gender_data.sample(frac=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzwUXA0vJ9vn",
        "colab_type": "text"
      },
      "source": [
        "## Analysing the Dataset\n",
        "\n",
        "In this step, I will perform some basic analysis on the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ukJmOpvI6Gf",
        "colab_type": "code",
        "outputId": "eb28e304-6d9d-467e-ef65-d04198d33da4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "# Priting the first 10 rows of the dataset\n",
        "gender_data.head()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>218301</th>\n",
              "      <td>Female</td>\n",
              "      <td>79827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>251441</th>\n",
              "      <td>Male</td>\n",
              "      <td>131406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>207734</th>\n",
              "      <td>Male</td>\n",
              "      <td>162302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38632</th>\n",
              "      <td>Female</td>\n",
              "      <td>39542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>189369</th>\n",
              "      <td>Female</td>\n",
              "      <td>125931</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        gender    path\n",
              "218301  Female   79827\n",
              "251441    Male  131406\n",
              "207734    Male  162302\n",
              "38632   Female   39542\n",
              "189369  Female  125931"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oA_TpuzVJAF2",
        "colab_type": "code",
        "outputId": "7c15f48b-db0c-4aee-bab1-2ddadbb43e1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "# Printing the last 10 rows of the dataset\n",
        "gender_data.tail()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>118893</th>\n",
              "      <td>Female</td>\n",
              "      <td>256257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174672</th>\n",
              "      <td>Male</td>\n",
              "      <td>121604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>267055</th>\n",
              "      <td>Male</td>\n",
              "      <td>123124</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55371</th>\n",
              "      <td>Female</td>\n",
              "      <td>188092</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>219443</th>\n",
              "      <td>Male</td>\n",
              "      <td>72717</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        gender    path\n",
              "118893  Female  256257\n",
              "174672    Male  121604\n",
              "267055    Male  123124\n",
              "55371   Female  188092\n",
              "219443    Male   72717"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5BZ-ap1JFNi",
        "colab_type": "code",
        "outputId": "4973bf83-a784-4c5a-e44a-6ea7a64c77b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Listing the column names\n",
        "print(gender_data.columns)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['gender', 'path'], dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_nMitPQJZzv",
        "colab_type": "text"
      },
      "source": [
        "The column gender contains the gender label as Male and Female. And the column path contains the unique id of the images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6ypSuGDJMsz",
        "colab_type": "code",
        "outputId": "d11fc5ec-32e8-4d77-8a8b-6eb22bb4153d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Number of records present on the data\n",
        "gender_data.shape"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(26801, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19QqVpb1KIxr",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing the Dataset\n",
        "\n",
        "Here I will perform some basic analysis of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYhGGdpSJWeS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The path columns contains int values. I need to change it to string\n",
        "gender_data = gender_data.astype({'gender' : str, 'path' : str})\n",
        "\n",
        "# Add the .jpg image extension after the id of the image\n",
        "gender_data['path'] = gender_data['path'] + '.jpg'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_g0lo2RjKmR1",
        "colab_type": "code",
        "outputId": "a2dda973-8764-48d6-e7df-03b863d2f1c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "# Check the data again\n",
        "gender_data.head()"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>218301</th>\n",
              "      <td>Female</td>\n",
              "      <td>79827.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>251441</th>\n",
              "      <td>Male</td>\n",
              "      <td>131406.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>207734</th>\n",
              "      <td>Male</td>\n",
              "      <td>162302.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38632</th>\n",
              "      <td>Female</td>\n",
              "      <td>39542.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>189369</th>\n",
              "      <td>Female</td>\n",
              "      <td>125931.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        gender        path\n",
              "218301  Female   79827.jpg\n",
              "251441    Male  131406.jpg\n",
              "207734    Male  162302.jpg\n",
              "38632   Female   39542.jpg\n",
              "189369  Female  125931.jpg"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7GEL9B9OcR_",
        "colab_type": "text"
      },
      "source": [
        "## Checking the class balance \n",
        "\n",
        "In this step, I need to check the class balance of the dataset. If there is more samples for one class, then our model can be biased towards that class. To solve this problem, we need to use some type of sampling. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ac-P5rE-Ozgb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "outputId": "e213c00f-217c-4980-f556-0f155f4d0b8c"
      },
      "source": [
        "genders = []\n",
        "\n",
        "for gender in gender_data['gender'].values:\n",
        "  if gender == 'Male':\n",
        "    genders.append(1)\n",
        "  else:\n",
        "    genders.append(0)\n",
        "\n",
        "plt.hist(genders, 2)\n",
        "plt.show()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSR3DQZLU+X8z999E5xMmBAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVBoX_pQOcNo",
        "colab_type": "text"
      },
      "source": [
        "Here there is almost equal number of samples for each class. So we dont need to do any sampling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOuwfDE-KxSi",
        "colab_type": "text"
      },
      "source": [
        "## Spliting the dataset\n",
        "\n",
        "Here I will split the dataset into two parts, one for training and one for testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9MFuebvKqWD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train, test = train_test_split(gender_data, test_size=0.2, random_state=1969)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWlCcCeKLqMC",
        "colab_type": "code",
        "outputId": "70b10f00-c8e0-441e-f49e-61d6497eaa2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Checking the data again\n",
        "print(train.shape)\n",
        "print(test.shape)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(21440, 2)\n",
            "(5361, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4BozPnbLxnH",
        "colab_type": "code",
        "outputId": "9c88618f-b2eb-483a-928e-b35449224842",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "test.head()"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>245386</th>\n",
              "      <td>Male</td>\n",
              "      <td>29439.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15950</th>\n",
              "      <td>Male</td>\n",
              "      <td>54752.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>235187</th>\n",
              "      <td>Male</td>\n",
              "      <td>44825.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77193</th>\n",
              "      <td>Female</td>\n",
              "      <td>138479.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>206849</th>\n",
              "      <td>Male</td>\n",
              "      <td>95278.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        gender        path\n",
              "245386    Male   29439.jpg\n",
              "15950     Male   54752.jpg\n",
              "235187    Male   44825.jpg\n",
              "77193   Female  138479.jpg\n",
              "206849    Male   95278.jpg"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QSoMnafLzaa",
        "colab_type": "code",
        "outputId": "d3635910-0154-445b-9b10-b816171402dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "train.tail()"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>137064</th>\n",
              "      <td>Male</td>\n",
              "      <td>83523.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132819</th>\n",
              "      <td>Female</td>\n",
              "      <td>254489.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>194686</th>\n",
              "      <td>Male</td>\n",
              "      <td>19205.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>138045</th>\n",
              "      <td>Male</td>\n",
              "      <td>151468.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16652</th>\n",
              "      <td>Female</td>\n",
              "      <td>141163.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        gender        path\n",
              "137064    Male   83523.jpg\n",
              "132819  Female  254489.jpg\n",
              "194686    Male   19205.jpg\n",
              "138045    Male  151468.jpg\n",
              "16652   Female  141163.jpg"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjVWKiAuMwdy",
        "colab_type": "text"
      },
      "source": [
        "## Generator Functions\n",
        "\n",
        "The dataset in big, So we need to read the data in small batch. In Keras, ImageDataGenerator class provides a generator methods that we can use here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcOp8XkLMsSJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A generator object with some basic settings\n",
        "generator = ImageDataGenerator(featurewise_center=False,\n",
        "                               samplewise_center=False,  \n",
        "                               featurewise_std_normalization=False,\n",
        "                               samplewise_std_normalization=False,\n",
        "                               zca_whitening=False,\n",
        "                               rotation_range=10,  \n",
        "                               zoom_range = 0.1, \n",
        "                               width_shift_range=0.1,  \n",
        "                               height_shift_range=0.1,\n",
        "                               horizontal_flip=True,  \n",
        "                               vertical_flip=False)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIUK4ALtNVZt",
        "colab_type": "code",
        "outputId": "33828745-ccb0-4602-9b2e-4d172112efdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Now I will read the dataset using the generator \n",
        "train_gen = generator.flow_from_dataframe(train, \n",
        "                                          directory='images/',\n",
        "                                          x_col='path',\n",
        "                                          y_col='gender',\n",
        "                                          target_size=(224,224),\n",
        "                                          batch_size=64)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 21440 validated image filenames belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2JQEFYJNuvq",
        "colab_type": "code",
        "outputId": "852a3d98-f91a-42ae-a36c-3d0803420049",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "test_gen = generator.flow_from_dataframe(test, \n",
        "                                         directory='images/',\n",
        "                                         x_col='path',\n",
        "                                         y_col='gender',\n",
        "                                         target_size=(224,224),\n",
        "                                         batch_size=64)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 5361 validated image filenames belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2R-v0BjN68K",
        "colab_type": "text"
      },
      "source": [
        "## Model\n",
        "\n",
        "Here I will make the MobileNetV2 model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJVJGSazN2I7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "# Im initializing the model with imagenet weighs\n",
        "mobile = MobileNetV2(include_top=False,\n",
        "                     weights=\"imagenet\", \n",
        "                     input_shape=(224,224,3),\n",
        "                     pooling=\"max\")\n",
        "\n",
        "model.add(mobile)\n",
        "model.add(Dense(units=2, activation=\"softmax\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehcL1HrsOa0d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer=optimizers.RMSprop(lr=2e-5), metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DkBdWczOfD5",
        "colab_type": "code",
        "outputId": "c71165fe-6804-4df4-b4e0-3592712a3e45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "mobilenetv2_1.00_224 (Model) (None, 1280)              2257984   \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 2)                 2562      \n",
            "=================================================================\n",
            "Total params: 2,260,546\n",
            "Trainable params: 2,226,434\n",
            "Non-trainable params: 34,112\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nf51buAcOp4F",
        "colab_type": "text"
      },
      "source": [
        "## Training the Model\n",
        "\n",
        "Here I will train the model with the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsX4lFQ3Og1e",
        "colab_type": "code",
        "outputId": "6f88de2e-1f77-4bc4-a311-a63760a2c622",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        }
      },
      "source": [
        "STEP_SIZE_TRAIN=train_gen.n//train_gen.batch_size\n",
        "STEP_SIZE_TEST=test_gen.n//test_gen.batch_size\n",
        "\n",
        "history = model.fit_generator(train_gen,\n",
        "                              steps_per_epoch=STEP_SIZE_TRAIN,\n",
        "                              validation_data=test_gen,\n",
        "                              validation_steps=STEP_SIZE_TEST,\n",
        "                              epochs=2)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "335/335 [==============================] - 445s 1s/step - loss: 0.8326 - acc: 0.7554 - val_loss: 0.6658 - val_acc: 0.7989\n",
            "Epoch 2/2\n",
            "335/335 [==============================] - 400s 1s/step - loss: 0.6194 - acc: 0.8097 - val_loss: 0.6480 - val_acc: 0.8003\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYC11iU1PbpC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('weights.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWsSrI17uywL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}